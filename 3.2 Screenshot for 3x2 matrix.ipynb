{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Screenshot for 3x2 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "with open (\"SAKI Exercise 3 warehousetraining2x2.txt\",\"r\") as inputpar:\n",
    "    trainingSet = inputpar.readlines()\n",
    "    \n",
    "cleaned_training = []\n",
    "for i, x in enumerate(trainingSet):\n",
    "    y = x.replace(\"\\t\", \"\")\n",
    "    y = y.replace(\"\\n\", \"\")\n",
    "    cleaned_training.append(y)\n",
    "\n",
    "# Probabilities for each subsequent state\n",
    "listStoreWhite =[]\n",
    "listStoreRed = []\n",
    "listStoreBlue = []\n",
    "listRestoreWhite = []\n",
    "listRestoreRed = []\n",
    "listRestoreBlue = []\n",
    "together = [listStoreRed,  listStoreBlue, listStoreWhite, listRestoreRed, listRestoreBlue, listRestoreWhite,]\n",
    "for i, entry in enumerate(cleaned_training):\n",
    "    if i < len(cleaned_training)-1:\n",
    "        if entry == \"storered\":\n",
    "                listStoreRed.append(cleaned_training[i+1])\n",
    "        elif entry == \"storeblue\":\n",
    "                listStoreBlue.append(cleaned_training[i+1])\n",
    "        elif entry == \"storewhite\":\n",
    "                listStoreWhite.append(cleaned_training[i+1])\n",
    "        elif entry == \"restorered\":\n",
    "                listRestoreRed.append(cleaned_training[i+1])\n",
    "        elif entry == \"restoreblue\":\n",
    "                listRestoreBlue.append(cleaned_training[i+1])\n",
    "        elif entry == \"restorewhite\":\n",
    "                listRestoreWhite.append(cleaned_training[i+1])\n",
    "\n",
    "action_col_combination = []\n",
    "colour = [\"red\", \"blue\", \"white\"]\n",
    "store_restore = [\"store\", \"restore\"]\n",
    "for act in store_restore:\n",
    "        for col in colour:\n",
    "            action_col_combination.append(act+col)\n",
    "\n",
    "anzahlDict = {}\n",
    "trainingsDict = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "sums = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "\n",
    "for i, x in enumerate(together):\n",
    "        for ind in action_col_combination:\n",
    "            anzahlDict[ind] = x.count(ind)\n",
    "        for summe in anzahlDict:\n",
    "            sums[i] += anzahlDict[summe]\n",
    "        trainingsDict[i] = copy.deepcopy(anzahlDict)\n",
    "\n",
    "percentageDict = copy.deepcopy(trainingsDict)\n",
    "\n",
    "# Derivation of probabilites\n",
    "for i, o in enumerate(trainingsDict):\n",
    "    for x in action_col_combination:\n",
    "        percentageDict[i][x] = trainingsDict[i][x]/sums[i]\n",
    "for i in range(0, 6):\n",
    "    percentageDict[action_col_combination[i]] = percentageDict[i]\n",
    "    del percentageDict[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up State-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24576\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "fields = [(1, 1), (1, 2), (2, 1), (2, 2), (3,1), (3,2)]\n",
    "\n",
    "field_combination = []\n",
    "for i in range(0, len(fields)+1):\n",
    "    field_combination.append(list(itertools.combinations(fields, i)))\n",
    "    \n",
    "\n",
    "def combinations_with_replacement(iterable, r):\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    for indices in itertools.product(range(n), repeat=r):\n",
    "        yield tuple(pool[i] for i in indices)\n",
    "\n",
    "sr_space = []\n",
    "for act in store_restore:\n",
    "    for col in colour:\n",
    "        sr_space.append([act, col])\n",
    "\n",
    "color_combinations_state = []\n",
    "for i in range(0, len(fields)+1):\n",
    "        color_combinations_state.append(list(combinations_with_replacement(colour, i)))\n",
    "\n",
    "final_states = []\n",
    "for sr in sr_space:\n",
    "        for i, field in enumerate(field_combination):\n",
    "                for entry in field:\n",
    "                        for col in color_combinations_state[i]:\n",
    "                                final_states.append((entry, col, sr))                                \n",
    "print(len(final_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Transition Probability Matrix and Reward Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix intialized\n",
      "24576\n",
      "(1, 1)\n",
      "end of action\n",
      "(1, 2)\n",
      "end of action\n",
      "(2, 1)\n",
      "end of action\n",
      "(2, 2)\n",
      "end of action\n",
      "(3, 1)\n",
      "end of action\n",
      "(3, 2)\n",
      "end of action\n",
      "end of transprob and reward matrix\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "lookup_mtrx = {(1, 1): 8, (1, 2): 6, (2, 1): 6, (2, 2): 4, (3, 1): 4, (3, 2): 2}\n",
    "\n",
    "trans_prob_matrix = []\n",
    "reward_matrix = []\n",
    "for i in range(len(fields)):\n",
    "        trans_prob = np.zeros((len(final_states), len(final_states)), dtype=np.float16)\n",
    "        reward = np.full((len(final_states), len(final_states)), -6, dtype=np.int8)\n",
    "        trans_prob_matrix.append(trans_prob)\n",
    "        reward_matrix.append(reward)\n",
    "print(\"matrix intialized\")\n",
    "print(len(trans_prob_matrix[0]))\n",
    "\n",
    "\n",
    "def FindNewCoordinate(stateA, stateB):\n",
    "        a = 0\n",
    "\n",
    "        stateA = list(map(list, stateA))\n",
    "        stateB = list(map(list, stateB))\n",
    "\n",
    "        while a != len(stateA[0]):\n",
    "                coordA = stateA[0][a]\n",
    "                if coordA in stateB[0]:\n",
    "                        indexB = stateB[0].index(coordA)\n",
    "                        if stateA[1][a] == stateB[1][indexB]:\n",
    "                                del stateA[0][a]\n",
    "                                del stateB[0][indexB]\n",
    "                                del stateA[1][a]\n",
    "                                del stateB[1][indexB]\n",
    "                                a -= 1\n",
    "                a += 1\n",
    "\n",
    "        if len(stateA[0]) == 0:\n",
    "                stateB[0] = tuple(stateB[0])\n",
    "                stateB[1] = tuple(stateB[1])\n",
    "                return stateB, len(stateB[0])\n",
    "        elif len(stateB[0]) == 0:\n",
    "                stateA[0] = tuple(stateA[0])\n",
    "                stateA[1] = tuple(stateA[1])\n",
    "                return stateA, len(stateA[0])\n",
    "        return False\n",
    "\n",
    "\n",
    "# set up transition probability and reward matrix\n",
    "for a, action in enumerate(fields):\n",
    "        print(action)\n",
    "        for y, row in enumerate(final_states):\n",
    "                potential_illegal_states = []\n",
    "                for x, col in enumerate(final_states):\n",
    "                        if row[2][0] == \"store\":\n",
    "                                if len(row[0])+1 == len(col[0]):\n",
    "                                        helfer = FindNewCoordinate(row, col)\n",
    "                                        if helfer is not False:\n",
    "                                                if helfer[0][0][0] == action and helfer[0][1][0] == row[2][1]:\n",
    "                                                        trans_prob_matrix[a][y][x] = percentageDict[row[2][0]+row[2][1]][col[2][0]+col[2][1]]\n",
    "                                                        reward_matrix[a][y][x] = lookup_mtrx[action]\n",
    "                                elif len(row[0]) == len(col[0]):\n",
    "                                        helfer = FindNewCoordinate(row, col)\n",
    "                                        if helfer is not False and helfer[1] == 0:\n",
    "                                                potential_illegal_states.append(x)\n",
    "\n",
    "                        if row[2][0] == \"restore\":\n",
    "                                if len(row[0])-1 == len(col[0]):\n",
    "                                        helfer = FindNewCoordinate(row, col)\n",
    "                                        if helfer is not False:\n",
    "                                                if helfer[0][0][0] == action and helfer[0][1][0] == row[2][1]:\n",
    "                                                        trans_prob_matrix[a][y][x] = percentageDict[row[2][0]+row[2][1]][col[2][0]+col[2][1]]\n",
    "                                                        reward_matrix[a][y][x] = lookup_mtrx[action]\n",
    "                                elif len(row[0]) == len(col[0]):\n",
    "                                        helfer = FindNewCoordinate(row, col)\n",
    "                                        if helfer is not False and helfer[1] == 0:\n",
    "                                                potential_illegal_states.append(x)\n",
    "\n",
    "                if np.sum(trans_prob_matrix[a][y], dtype=np.float16) == 0:\n",
    "                        assert len(potential_illegal_states) == 6\n",
    "                        for candidates in potential_illegal_states:\n",
    "                                 trans_prob_matrix[a][y][candidates] = percentageDict[row[2][0]+row[2][1]][final_states[candidates][2][0]+final_states[candidates][2][1]]\n",
    "\n",
    "                if np.sum(trans_prob_matrix[a][y], dtype=np.float16) != 1:\n",
    "                        tmp = np.float16(1.0)-np.sum(trans_prob_matrix[a][y], dtype=np.float16)\n",
    "                        can = np.argwhere(x > 0)\n",
    "                        trans_prob_matrix[a][y][can[-1]] += tmp\n",
    "                assert np.sum(trans_prob_matrix[a][y], dtype=np.float16) == 1.0\n",
    "\n",
    "        #pd.DataFrame(trans_prob_matrix[a]).to_csv(str(action)+\"transProbMatrix_adap_2x2.csv\", sep=\";\", header=final_states, decimal=\".\")\n",
    "        #pd.DataFrame(reward_matrix[a]).to_csv(str(action)+\"rewardMatrix_adap_2x2.csv\", sep=\";\", header=final_states, decimal=\".\")\n",
    "        print(\"end of action\")\n",
    "print(\"end of transprob and reward matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Derivation using MDP Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymdptoolbox in c:\\programdata\\anaconda3\\lib\\site-packages (4.0b3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from pymdptoolbox) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from pymdptoolbox) (1.14.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "smart-open 1.6.0 requires bz2file, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pymdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3521.594519853592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "ts_beginning = time.time()\n",
    "pi_vigs = mdptoolbox.mdp.RelativeValueIteration(trans_prob_matrix, reward_matrix)\n",
    "pi_vigs.run()\n",
    "x = list(pi_vigs.policy)\n",
    "pd.DataFrame(x).to_csv(\"RelativeValueIteration3x2.csv\", sep=\";\")\n",
    "ts_end = time.time()\n",
    "time_difference = ts_end-ts_beginning\n",
    "print(time_difference)\n",
    "pi_vigs.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8178.164109706879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mdptoolbox\n",
    "ts_beginning = time.time()\n",
    "pi_vigs = mdptoolbox.mdp.ValueIteration(trans_prob_matrix, reward_matrix, 0.99)\n",
    "pi_vigs.run()\n",
    "pd.DataFrame(list(pi_vigs.policy)).to_csv(\"policyValueIteration0.993x2.csv\", sep=\";\")\n",
    "ts_end = time.time()\n",
    "time_difference = ts_beginning - ts_end\n",
    "print(time_difference)\n",
    "pi_vigs.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-afc5e2f84d33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpi_vigs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdptoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mValueIterationGS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_prob_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpi_vigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi_vigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"policyValueIterationGS3x2.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpi_vigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\mdptoolbox\\mdp.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1502\u001b[0m                 Q = [float(self.R[a][s]+\n\u001b[0;32m   1503\u001b[0m                            self.discount * self.P[a][s, :].dot(self.V))\n\u001b[1;32m-> 1504\u001b[1;33m                      for a in range(self.A)]\n\u001b[0m\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\mdptoolbox\\mdp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1502\u001b[0m                 Q = [float(self.R[a][s]+\n\u001b[0;32m   1503\u001b[0m                            self.discount * self.P[a][s, :].dot(self.V))\n\u001b[1;32m-> 1504\u001b[1;33m                      for a in range(self.A)]\n\u001b[0m\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pi_vigs = mdptoolbox.mdp.ValueIterationGS(trans_prob_matrix, reward_matrix, 0.999)\n",
    "pi_vigs.run()\n",
    "pd.DataFrame(pi_vigs.policy).to_csv(\"policyValueIterationGS3x2.csv\", sep=\";\")\n",
    "pi_vigs.iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Warehouse Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"SAKI Exercise 3 warehouseordernew.txt\",\"r\") as inputpar:\n",
    "    trainingSet = inputpar.readlines()\n",
    "\n",
    "iter_trainingsset= []\n",
    "for i, x in enumerate(trainingSet):\n",
    "        y = x.replace(\"\\t\", \" \")\n",
    "        y = y.replace(\"\\n\", \"\")\n",
    "        y = y.split()\n",
    "        iter_trainingsset.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_greedy = {(1, 1): 1, (1, 2): 2, (2, 1): 2, (2, 2): 3, (3, 1): 3, (3, 2): 4}\n",
    "greedy_matrix = [[0, \"empty\"], [0, \"empty\"], [0, \"empty\"], [0, \"empty\"], [0, \"empty\"], [0, \"empty\"]]\n",
    "eval_greedy_algorithm = []\n",
    "for row in iter_trainingsset:\n",
    "    if row[0] == \"store\":\n",
    "        for i, action in enumerate(fields):\n",
    "            if greedy_matrix[i][0] == 0:\n",
    "                greedy_matrix[i][0] = 1\n",
    "                greedy_matrix[i][1] = row[1]\n",
    "                eval_greedy_algorithm.append([row, action, distance_greedy[action]])\n",
    "                break\n",
    "    elif row[0] == \"restore\":        \n",
    "        for i, action in enumerate(fields):\n",
    "            if greedy_matrix[i][0] == 1 and row[1] == greedy_matrix[i][1]:\n",
    "                greedy_matrix[i][0] = 0\n",
    "                greedy_matrix[i][1] = \"empty\"\n",
    "                eval_greedy_algorithm.append([row, action, distance_greedy[action]])\n",
    "                break\n",
    "pd.DataFrame(eval_greedy_algorithm).to_csv(\"eval_greedy_algorithm.csv\", sep=\";\")\n",
    "\n",
    "print(eval_greedy_algorithm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of MDP policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"ModifiedPolicyIteration.csv\", \"r\") as inputpar:\n",
    "    policy = inputpar.readlines()\n",
    "\n",
    "cleaned_policy = []\n",
    "for i, x in enumerate(policy):\n",
    "    if i > 0:\n",
    "        if len(cleaned_policy) == 9 or len(cleaned_policy) == 99 or len(cleaned_policy) == 999 or len(cleaned_policy) == 9999:\n",
    "            y = x[len(str(i)):]\n",
    "            y = y.replace(\"\\n\", \"\")\n",
    "            cleaned_policy.append(y)\n",
    "        else:\n",
    "            y = x[len(str(i))+1:]\n",
    "            y = y.replace(\"\\n\", \"\")\n",
    "            cleaned_policy.append(y)\n",
    "\n",
    "\n",
    "def lookup_candidate(matrix, stateB):\n",
    "    list_coord = []\n",
    "    list_colour = []\n",
    "    for i in matrix:\n",
    "        if i[1] != \"empty\":\n",
    "            list_coord.append(i[2])\n",
    "            list_colour.append(i[1])\n",
    "    if len(list_coord) != len(stateB[0]):\n",
    "        return False\n",
    "    for i in range(0, len(list_coord)):\n",
    "        if list_coord[i] != stateB[0][i] or list_colour[i] != stateB[1][i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "rl_matrix = [[0, \"empty\", (1, 1)], [0, \"empty\", (1, 2)], [0, \"empty\", (2, 1)], [0, \"empty\", (2, 2)], [0, \"empty\", (3, 1)], [0, \"empty\", (3, 2)]]\n",
    "distance_rl = {\"0\": 1, \"1\": 2, \"2\": 3, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "rl_algorithm = []\n",
    "\n",
    "\n",
    "def empty_counter(matrix):\n",
    "    counter = 0\n",
    "    for entry in rl_matrix:\n",
    "        if entry[1] != \"empty\":\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def inner_func():\n",
    "    for state_index, row_states in enumerate(final_states):\n",
    "            if row_states[2][0] == row[0] and row_states[2][1] == row[1]:\n",
    "                            if not(rl_matrix[0][1] == rl_matrix[1][1] == rl_matrix[2][1] == rl_matrix[3][1] == \"empty\"):\n",
    "                                    count = 0\n",
    "                                    for it in range(0, len(row_states[0])):\n",
    "                                            if rl_matrix[it][1] == \"empty\" and row_states[1][it] != \"\":\n",
    "                                                    if lookup_candidate(rl_matrix, row_states) is True and row[0] == \"store\":\n",
    "                                                                    step_counter = str(cleaned_policy[state_index])\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][0] = 1\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][1] = row[1]\n",
    "                                                                    rl_algorithm.append([row, rl_matrix[int(cleaned_policy[state_index])][2], distance_rl[step_counter]])\n",
    "                                                                    return\n",
    "                                                    if lookup_candidate(rl_matrix, row_states) is True and row[0] == \"restore\":\n",
    "                                                                    step_counter = str(cleaned_policy[state_index])\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][0]= 0\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][1]=\"empty\"\n",
    "                                                                    rl_algorithm.append([row,rl_matrix[int(cleaned_policy[state_index])][2],distance_rl[step_counter]])\n",
    "                                                                    return\n",
    "                                                    else:\n",
    "                                                            continue                                                                                                                      \n",
    "                                            if rl_matrix[it][2] == row_states[0][it] and rl_matrix[it][1] == row_states[1][it]:\n",
    "                                                    count += 1\n",
    "                                                    if count == len(row_states[0]) == empty_counter(rl_matrix):\n",
    "                                                            if row[0] == \"store\":\n",
    "                                                                    step_counter = str(cleaned_policy[state_index])\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][0] = 1\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][1] = row[1]\n",
    "                                                                    rl_algorithm.append([row, rl_matrix[int(cleaned_policy[state_index])][2], distance_rl[step_counter]])\n",
    "                                                                    return\n",
    "                                                            elif row[0] == \"restore\":\n",
    "                                                                    step_counter = str(cleaned_policy[state_index])\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][0] = 0\n",
    "                                                                    rl_matrix[int(cleaned_policy[state_index])][1] =\"empty\"\n",
    "                                                                    rl_algorithm.append([row, rl_matrix[int(cleaned_policy[state_index])][2], distance_rl[step_counter]])\n",
    "                                                                    return\n",
    "                            # case empty matrix\n",
    "                            else:\n",
    "                                    if row[0] == row_states[2][0] and row[1] == row_states[2][1]:\n",
    "                                            if row[0] == \"store\":\n",
    "                                                    step_counter = str(cleaned_policy[state_index])\n",
    "                                                    rl_matrix[int(cleaned_policy[state_index])][0] = 1\n",
    "                                                    rl_matrix[int(cleaned_policy[state_index])][1] = row[1]\n",
    "                                                    rl_algorithm.append([row, rl_matrix[int(cleaned_policy[state_index])][2], distance_rl[step_counter]])\n",
    "                                                    return\n",
    "                                            elif row[0] == \"restore\":\n",
    "                                                    step_counter = str(cleaned_policy[state_index])\n",
    "                                                    rl_matrix[int(cleaned_policy[state_index])][0] = 0\n",
    "                                                    rl_matrix[int(cleaned_policy[state_index])][1] = \"empty\"\n",
    "                                                    rl_algorithm.append([row, rl_matrix[int(cleaned_policy[state_index])][2], distance_rl[step_counter]])\n",
    "                                                    return\n",
    "\n",
    "for j, row in enumerate(iter_trainingsset):\n",
    "    inner_func()\n",
    "\n",
    "pd.DataFrame(rl_algorithm).to_csv(\"rl_algorithm.csv\", sep=\";\")\n",
    "print(rl_algorithm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
